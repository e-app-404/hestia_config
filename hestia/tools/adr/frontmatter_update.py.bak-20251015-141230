#!/usr/bin/env python3

"""
ADR Frontmatter Update Orchestrator

Coordinates individual field processors for comprehensive ADR frontmatter management.
Systematically updates all ADR files to include required frontmatter fields:
- id, title, slug, status, related, supersedes, last_updated, date, decision

Uses modular field-specific processors defined in the canonical meta-structure.
"""

import argparse
import re
import shutil
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# Try to import toml, fallback if not available
try:
    import toml
except ImportError:
    toml = None

try:
    import yaml
except ImportError:
    print("ERROR: pyyaml is required.")
    sys.exit(1)

SCRIPT_DIR = Path(__file__).parent
ADR_DIR = Path("/config/hestia/library/docs/ADR")
META_CONFIG_PATH = Path("/config/hestia/config/meta/adr.toml")

REQUIRED_FIELDS = [
    "id",
    "title",
    "slug",
    "status",
    "related",
    "supersedes",
    "last_updated",
    "date",
    "decision",
]
OPTIONAL_FIELDS = ["related", "supersedes"]

DEFAULT_FIELD_ORDER = [
    "id",
    "title",
    "slug",
    "status",
    "related",
    "supersedes",
    "last_updated",
    "date",
    "decision",
]


def generate_slug(title: str) -> str:
    """Generate kebab-case slug from title, removing ADR-XXXX prefix"""
    slug_base = re.sub(r"^ADR-\d+:\s*", "", title, flags=re.IGNORECASE)
    slug = re.sub(r"[^\w\s-]", "", slug_base.lower())
    slug = re.sub(r"[-\s]+", "-", slug)
    return slug.strip("-")


def extract_decision_summary(content: str) -> str:
    """Extract decision summary from document content"""
    decision_patterns = [
        r"##\s*(?:\d+\.\s*)?Decision\s*\n\n(.*?)(?=\n##|\n```|\Z)",
        r"##\s*(?:\d+\.\s*)?Decision\s*\n(.*?)(?=\n##|\n```|\Z)",
    ]
    for pattern in decision_patterns:
        match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        if match:
            decision_text = match.group(1).strip()
            decision_text = re.sub(r"\n+", " ", decision_text)
            decision_text = re.sub(r"\s+", " ", decision_text)
            if len(decision_text) <= 200:
                return decision_text
            sentences = re.split(r"(?<=[.!?])\s+", decision_text)
            result = sentences[0]
            for i in range(1, len(sentences)):
                if len(result + " " + sentences[i]) <= 200:
                    result += " " + sentences[i]
                else:
                    break
            return result
    # Fallback: look for bullet points or any content after "Decision"
    decision_match = re.search(r"(?i)decision.*?[-*]\s*(.*?)(?=\n|$)", content)
    if decision_match:
        return decision_match.group(1).strip()[:200]
    # Last resort: extract from first paragraph
    first_para = re.search(
        r"(?:##\s*(?:\d+\.\s*)?(?:Context|Decision|Summary).*?\n\n)(.*?)(?=\n##|\n```|\Z)",
        content,
        re.DOTALL | re.IGNORECASE,
    )
    if first_para:
        para_text = first_para.group(1).strip()
        para_text = re.sub(r"\n+", " ", para_text)
        para_text = re.sub(r"\s+", " ", para_text)
        return para_text[:200] + "..." if len(para_text) > 200 else para_text
    return "Architectural decision documented in this ADR."


class FrontmatterOrchestrator:
    def __init__(self):
        self.field_order = DEFAULT_FIELD_ORDER
        self.processors = {}
        self.meta_config = None
        self.discover_processors()

    def discover_processors(self):
        """Find available field processor scripts"""
        for field in self.field_order:
            processor_path = SCRIPT_DIR / f"frontmatter-{field}.py"
            if processor_path.exists():
                self.processors[field] = processor_path

    def run_processor(self, field, files, dry_run=False, backup=False, validate_only=False):
        """Run a single field processor"""
        if field not in self.processors:
            print(f"ERROR: No processor available for field '{field}'")
            return False
        processor_path = self.processors[field]
        cmd = [sys.executable, str(processor_path)]
        if files:
            cmd.extend(files)
        if dry_run:
            cmd.append("--dry-run")
        if backup:
            cmd.append("--backup")
        if validate_only:
            cmd.append("--validate-only")
        try:
            print(f"\n{'=' * 60}")
            print(f"Running processor: {field}")
            print(f"{'=' * 60}")
            result = subprocess.run(cmd)
            return result.returncode == 0
        except Exception as e:
            print(f"ERROR: Failed to run processor for '{field}': {e}")
            return False

    def process_files(
        self, files, dry_run=False, backup=False, validate_only=False, selected_field=None
    ):
        """Coordinate processing across fields"""
        # Determine files to process
        if not files:
            file_paths = sorted(ADR_DIR.glob("ADR-*.md"))
        else:
            file_paths = [str(Path(f).resolve()) for f in files]
        if not file_paths:
            print("No ADR files found to process")
            return 1
        # Load meta-configuration if available
        if toml and META_CONFIG_PATH.exists():
            try:
                self.meta_config = toml.load(META_CONFIG_PATH)
                self.field_order = self.meta_config.get("processing", {}).get(
                    "field_order", DEFAULT_FIELD_ORDER
                )
            except Exception as e:
                print(f"WARNING: Could not load meta-configuration: {e}")
        print("ADR Frontmatter Update Orchestrator")
        print("=" * 50)
        print(f"Processing {len(file_paths)} ADR files")
        print(f"Mode: {'DRY-RUN' if dry_run else 'LIVE'}")
        fields_to_process = [selected_field] if selected_field else self.field_order
        success_count = 0
        error_count = 0
        self.discover_processors()
        for field in fields_to_process:
            if field not in self.processors:
                print(f"\nSKIPPING: No processor for field '{field}'")
                continue
            success = self.run_processor(field, file_paths, dry_run, backup, validate_only)
            if success:
                success_count += 1
            else:
                error_count += 1
        print("\nORCHESTRATOR SUMMARY:")
        print(f"  Fields processed: {success_count}")
        print(f"  Fields with errors: {error_count}")
        return error_count

    def list_processors(self):
        """List available field processors"""
        print("Available field processors:")
        for field in self.field_order:
            status = "✓" if field in self.processors else "✗"
            processor_path = SCRIPT_DIR / f"frontmatter-{field}.py"
            print(f"  {status} {field:15} -> {processor_path}")

    def validate_dependencies(self):
        """Check that all required processors are available"""
        missing_processors = []
        for field in self.field_order:
            if field not in self.processors:
                missing_processors.append(field)
        if missing_processors:
            print("Missing field processors:")
            for field in missing_processors:
                print(f"  - frontmatter-{field}.py")
            return False
        return True


def update_adr_frontmatter(file_path, dry_run=False, backup=False, dates_only=False):
    """Update a single ADR file's frontmatter"""
    print(f"\nProcessing {file_path.name}...")
    try:
        content = file_path.read_text(encoding="utf-8")
    except Exception as e:
        print(f"  ERROR: Could not read file: {e}")
        return False
    if not content.startswith("---"):
        print("  SKIP: No YAML frontmatter found")
        return False
    try:
        parts = content.split("---", 2)
        if len(parts) < 3:
            print("  ERROR: Invalid YAML frontmatter structure")
            return False
        yaml_content = parts[1]
        body_content = parts[2]
        frontmatter = yaml.safe_load(yaml_content) or {}
    except Exception as e:
        print(f"  ERROR: Could not parse YAML frontmatter: {e}")
        return False
    # Handle dates-only mode
    if dates_only:
        today = datetime.now().strftime("%Y-%m-%d")
        if "last_updated" in frontmatter and frontmatter["last_updated"] != today:
            new_frontmatter = frontmatter.copy()
            new_frontmatter["last_updated"] = today
            print(f"  UPDATE: last_updated -> {today}")
            if dry_run:
                print("  DRY-RUN: Would update file")
                return True
            if backup:
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                backup_path = file_path.with_suffix(f".md.bak-{timestamp}")
                shutil.copy2(file_path, backup_path)
                print(f"  BACKUP: Created {backup_path.name}")
            ordered_frontmatter = {
                k: new_frontmatter[k] for k in DEFAULT_FIELD_ORDER if k in new_frontmatter
            }
            for k, v in new_frontmatter.items():
                if k not in ordered_frontmatter:
                    ordered_frontmatter[k] = v
            try:
                new_yaml = yaml.dump(
                    ordered_frontmatter,
                    default_flow_style=False,
                    allow_unicode=True,
                    sort_keys=False,
                )
                new_content = f"---\n{new_yaml}---{body_content}"
                file_path.write_text(new_content, encoding="utf-8")
                print("  SUCCESS: File updated")
                return True
            except Exception as e:
                print(f"  ERROR: Could not write updated file: {e}")
                return False
        else:
            print("  OK: last_updated already current")
            return True
    # Check what's missing
    missing_fields = [field for field in REQUIRED_FIELDS if field not in frontmatter]
    updates_needed = []
    new_frontmatter = frontmatter.copy()
    # Generate ID from filename if missing
    if "id" in missing_fields:
        id_match = re.search(r"ADR-(\d+)", file_path.name)
        if id_match:
            new_frontmatter["id"] = f"ADR-{id_match.group(1)}"
            updates_needed.append(f"id: {new_frontmatter['id']}")
    # Generate slug if missing
    if "slug" in missing_fields and "title" in frontmatter:
        new_frontmatter["slug"] = generate_slug(frontmatter["title"])
        updates_needed.append(f"slug: {new_frontmatter['slug']}")
    # Add empty arrays for related/supersedes if missing
    if "related" in missing_fields:
        new_frontmatter["related"] = []
        updates_needed.append("related: []")
    if "supersedes" in missing_fields:
        new_frontmatter["supersedes"] = []
        updates_needed.append("supersedes: []")
    # Update last_updated to current date
    today = datetime.now().strftime("%Y-%m-%d")
    if "last_updated" in missing_fields or "last_updated" in frontmatter:
        new_frontmatter["last_updated"] = today
        updates_needed.append(f"last_updated: {new_frontmatter['last_updated']}")
    # Generate decision summary if missing
    if "decision" in missing_fields:
        decision_summary = extract_decision_summary(body_content)
        new_frontmatter["decision"] = decision_summary
        updates_needed.append(f'decision: "{decision_summary[:50]}..."')
    if updates_needed:
        print(f"  UPDATES: {'; '.join(updates_needed)}")
        if dry_run:
            print("  DRY-RUN: Would update file")
            return True
        if backup:
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            backup_path = file_path.with_suffix(f".md.bak-{timestamp}")
            shutil.copy2(file_path, backup_path)
            print(f"  BACKUP: Created {backup_path.name}")
        ordered_frontmatter = {
            k: new_frontmatter[k] for k in DEFAULT_FIELD_ORDER if k in new_frontmatter
        }
        for k, v in new_frontmatter.items():
            if k not in ordered_frontmatter:
                ordered_frontmatter[k] = v
        try:
            new_yaml = yaml.dump(
                ordered_frontmatter, default_flow_style=False, allow_unicode=True, sort_keys=False
            )
            new_content = f"---\n{new_yaml}---{body_content}"
            file_path.write_text(new_content, encoding="utf-8")
            print("  SUCCESS: File updated")
            return True
        except Exception as e:
            print(f"  ERROR: Could not write updated file: {e}")
            return False
    else:
        print("  OK: All required fields present")
        return True


def main():
    parser = argparse.ArgumentParser(
        description="ADR Frontmatter Standardization Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process all ADRs with dry-run
  %(prog)s --dry-run

  # Process specific files with backup
  %(prog)s ADR-0001.md ADR-0002.md --backup

  # Validate only the 'slug' field
  %(prog)s --field slug --validate-only

  # List available processors
  %(prog)s --list-processors
""",
    )
    parser.add_argument("files", nargs="*", help="ADR files to process (default: all)")
    parser.add_argument("--dry-run", action="store_true", help="Show changes without applying them")
    parser.add_argument(
        "--backup", action="store_true", help="Create backups before making changes"
    )
    parser.add_argument("--field", choices=DEFAULT_FIELD_ORDER, help="Process only specific field")
    parser.add_argument("--validate-only", action="store_true", help="Only validate, no changes")
    parser.add_argument(
        "--list-processors", action="store_true", help="List available field processors"
    )
    parser.add_argument(
        "--check-dependencies",
        action="store_true",
        help="Check that all required processors are available",
    )
    parser.add_argument(
        "--update-dates-only", action="store_true", help="Only update last_updated field to today"
    )
    args = parser.parse_args()

    print("ADR Frontmatter Standardization Tool")
    print("=" * 50)
    if args.dry_run:
        print("DRY-RUN MODE: No files will be modified")
    if args.update_dates_only:
        print("DATES-ONLY MODE: Only updating last_updated fields")

    if args.list_processors or args.check_dependencies or args.field or args.validate_only:
        orchestrator = FrontmatterOrchestrator()
        if args.list_processors:
            orchestrator.list_processors()
            return 0
        if args.check_dependencies:
            if orchestrator.validate_dependencies():
                print("All required field processors are available")
                return 0
            else:
                print("Some field processors are missing")
                return 1
        return orchestrator.process_files(
            args.files,
            dry_run=args.dry_run,
            backup=args.backup,
            validate_only=args.validate_only,
            selected_field=args.field,
        )

    adr_files = (
        sorted(ADR_DIR.glob("ADR-*.md")) if not args.files else [Path(f) for f in args.files]
    )
    if not adr_files:
        print("No ADR files found!")
        return 1

    print(f"Found {len(adr_files)} ADR files to process")
    success_count = 0
    error_count = 0
    for adr_file in adr_files:
        if update_adr_frontmatter(
            adr_file, dry_run=args.dry_run, backup=args.backup, dates_only=args.update_dates_only
        ):
            success_count += 1
        else:
            error_count += 1
    print("\nSUMMARY:")
    print(f"  Processed: {success_count}")
    print(f"  Errors: {error_count}")
    print(f"  Total: {len(adr_files)}")
    if not args.dry_run and success_count > 0:
        print("\nRun '/config/bin/adr-index' to regenerate the governance index")
    return 0 if error_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
